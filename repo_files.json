[{"path": ".gitignore", "content": "__pycache__/\n*.pyc\n*.pyd\n*.log\n.cache_sharp/\noutputs/\nuploads/\ndebug*/\ngsplat_build.log\nrun_full.log\nweb_server.log\n*.mp4\n*.mkv\n*.mov\n*.avi\n*.zip\n.env\n.venv/\n.idea/\n.vscode/\n"}, {"path": "README.md", "content": "# Video to SBS (Quest-friendly 3D)\n\n目标：把任意 2D 视频转换为 Quest 可舒适观看的高质量 3D 立体视频（SBS 左右并排 mp4）。\n\n## 安装\n\n步骤 1 / 安装 Python 依赖\n\n```bash\npython -m venv .venv\n.\\.venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n步骤 2 / 安装 ffmpeg\n\n确保 `ffmpeg` 在 PATH 中可用：\n\n```bash\nffmpeg -version\n```\n\n步骤 3 / 环境自检\n\n```bash\npython tools/env_check.py\n```\n\n## 一键运行\n\n```bash\npython tools/video_to_sbs.py --video input.mp4 --out output_sbs.mp4\n```\n\n## 可选前端\n\n启动本地前端（带运行按钮与进度条）：\n\n```bash\npython tools/web_server.py\n```\n\n浏览器访问 `http://127.0.0.1:7860`，输入视频会自动上传到本地缓存，输出完成后点击“下载输出”。\n\n## 关键参数（效果优先）\n\n- `--baseline_m`：默认 0.03m，舒适优先。\n- `--max_disp_px`：视差上限（像素），默认 30，越大越强烈但越容易眩晕。\n- `--fov_deg`：默认 60（水平 FOV）。若画面深度不对，优先调这个。\n- `--cut_threshold` / `--max_shot_len`：镜头切分稳定性。\n- `--debug_dir`：导出关键帧深度、采样 SBS、位姿日志。\n\n## Definition of Done（已在代码中检查/日志化）\n\n1) 输出质量（视觉）\n- 舒适视差：每帧基线做 disparity clamp（`--baseline_m` + `--max_disp_px`）。\n- 时序稳定：PnP 位姿轻量平滑；每帧记录 `inliers/reproj`。\n- 遮挡边界处理：基于 alpha 的轻量补洞（`--inpaint_radius`）。\n\n2) 稳健性\n- 自动镜头切分 + 失败恢复：直方图切分 + PnP 失败自动新关键帧。\n- 动态主体防污染：LK 光流 + Fundamental RANSAC + PnP RANSAC。\n\n3) 性能与工程质量\n- SHARP 仅关键帧调用，3DGS `.ply` 缓存。\n- ffmpeg pipe 流式编码（不落盘中间帧）。\n- 输出可诊断：每帧 log（inliers/reproj/new keyframe/render_ms），`--debug_dir` 导出关键结果。\n\n## 常见报错\n\n- CUDA 不可用：确保装了 CUDA 版 PyTorch，`python tools/env_check.py` 查看状态。\n- `ffmpeg` 不在 PATH：安装 ffmpeg 并配置环境变量。\n- SHARP 模型下载失败：设置代理或手动下载后用 `--sharp_ckpt` 指定。\n- 如果遇到 SSL 证书错误，程序会自动启用一次不校验证书的下载兜底。\n"}, {"path": "requirements.txt", "content": "git+https://github.com/apple/ml-sharp.git\nopencv-python\nflask"}, {"path": "start_web_ui.bat", "content": "@echo off\ncd /d \"%~dp0\"\nstart \"3D Server\" \"C:\\Users\\Q\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\" tools\\web_server.py\ntimeout /t 2 >nul\nstart \"\" \"http://127.0.0.1:7860\"\n"}, {"path": "tools/web_server.py", "content": "import json\nimport os\nimport logging\nimport queue\nimport re\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\n\nimport cv2\nfrom flask import Flask, Response, jsonify, request, send_file, send_from_directory\n\n\nROOT = Path(__file__).resolve().parents[1]\nWEB_DIR = ROOT / \"web\"\nLOG_PATH = ROOT / \"web_server.log\"\nUPLOADS_DIR = ROOT / \"uploads\"\nOUTPUTS_DIR = ROOT / \"outputs\"\n\nUPLOADS_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n    handlers=[\n        logging.FileHandler(LOG_PATH, encoding=\"utf-8\"),\n        logging.StreamHandler(),\n    ],\n)\nLOGGER = logging.getLogger(\"web_server\")\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ.setdefault(\"PYTHONUTF8\", \"1\")\nos.environ.setdefault(\"PYTHONIOENCODING\", \"utf-8\")\nos.environ.setdefault(\"TORCH_CUDA_ARCH_LIST\", \"12.0\")\n_CUDA_ROOT = Path(r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v13.0\")\n_CUDA_BIN = _CUDA_ROOT / \"bin\"\nif _CUDA_BIN.exists():\n    os.environ.setdefault(\"CUDA_PATH\", str(_CUDA_ROOT))\n    os.environ.setdefault(\"CUDA_HOME\", str(_CUDA_ROOT))\n    os.environ.setdefault(\"CUDA_ROOT\", str(_CUDA_ROOT))\n    os.environ[\"PATH\"] = f\"{_CUDA_BIN};{os.environ.get('PATH', '')}\"\n    os.environ.setdefault(\"GSPLAT_CUDA_HOME\", str(_CUDA_ROOT))\n\n_MSVC_ROOT = Path(r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\")\nif _MSVC_ROOT.exists():\n    cl_candidates = list(_MSVC_ROOT.glob(\"**/bin/Hostx64/x64/cl.exe\"))\n    if cl_candidates:\n        cl_bin = cl_candidates[0].parent\n        os.environ[\"PATH\"] = f\"{cl_bin};{os.environ.get('PATH', '')}\"\n\n\ndef safe_filename(name: str) -> str:\n    keep = []\n    for ch in name:\n        if ch.isalnum() or ch in (\"-\", \"_\", \".\"):\n            keep.append(ch)\n        else:\n            keep.append(\"_\")\n    cleaned = \"\".join(keep).strip(\"._\")\n    return cleaned or f\"upload_{int(time.time())}.mp4\"\n\napp = Flask(__name__, static_folder=str(WEB_DIR), static_url_path=\"\")\napp.config[\"SEND_FILE_MAX_AGE_DEFAULT\"] = 0\n\n\nclass Runner:\n    def __init__(self) -> None:\n        self.lock = threading.Lock()\n        self.proc = None\n        self.thread = None\n        self.heartbeat_thread = None\n        self.queue = queue.Queue()\n        self.total_frames = 0\n        self.current_frame = 0\n        self.running = False\n        self.stop_requested = False\n        self.last_output_path = \"\"\n        self.start_time = 0.0\n        self.last_output_time = 0.0\n        self.last_heartbeat = 0.0\n\n    def start(self, payload: dict) -> dict:\n        with self.lock:\n            if self.running:\n                return {\"ok\": False, \"error\": \"任务正在运行\"}\n\n            video = payload.get(\"video\")\n            out = payload.get(\"out\")\n            if not video:\n                return {\"ok\": False, \"error\": \"请先选择输入视频\"}\n\n            video_path = Path(video)\n            if not video_path.exists():\n                return {\"ok\": False, \"error\": \"输入视频不存在\"}\n\n            self.total_frames = self._get_total_frames(video_path)\n            self.current_frame = 0\n            self.queue = queue.Queue()\n            self.running = True\n            self.stop_requested = False\n            self.start_time = time.time()\n            self.last_output_time = self.start_time\n            self.last_heartbeat = 0.0\n\n            output_path = self._resolve_output(out)\n            self.last_output_path = output_path\n            payload[\"out\"] = output_path\n            log_path = OUTPUTS_DIR / f\"run_{int(time.time())}.log\"\n            payload[\"log_path\"] = str(log_path)\n            cmd = self._build_command(payload)\n\n            env = os.environ.copy()\n            env[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n            env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n            self.proc = subprocess.Popen(\n                cmd,\n                cwd=str(ROOT),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n                bufsize=1,\n                env=env,\n            )\n            self.thread = threading.Thread(target=self._reader, daemon=True)\n            self.thread.start()\n            self.heartbeat_thread = threading.Thread(target=self._heartbeat, daemon=True)\n            self.heartbeat_thread.start()\n            self._enqueue(\n                {\n                    \"type\": \"log\",\n                    \"line\": f\"任务已启动: video={video_path} out={output_path} frames={self.total_frames}\",\n                }\n            )\n            self._enqueue({\"type\": \"status\", \"state\": \"运行中\"})\n            return {\"ok\": True}\n\n    def stop(self) -> dict:\n        with self.lock:\n            if not self.running or self.proc is None:\n                return {\"ok\": False, \"error\": \"没有正在运行的任务\"}\n            self.stop_requested = True\n            self.proc.terminate()\n            return {\"ok\": True}\n\n    def stream(self):\n        while True:\n            try:\n                msg = self.queue.get(timeout=1)\n            except queue.Empty:\n                if self.running and (time.time() - self.last_heartbeat) > 5:\n                    elapsed = int(time.time() - self.start_time)\n                    self.last_heartbeat = time.time()\n                    yield f\"data: {json.dumps({'type': 'log', 'line': f'运行中... 已用时 {elapsed}s'}, ensure_ascii=False)}\\n\\n\"\n                else:\n                    yield \": ping\\n\\n\"\n                continue\n\n            yield f\"data: {json.dumps(msg, ensure_ascii=False)}\\n\\n\"\n            if msg.get(\"type\") == \"status\" and msg.get(\"state\") in {\"完成\", \"失败\", \"已停止\"}:\n                break\n\n    def _reader(self) -> None:\n        assert self.proc is not None\n        frame_re = re.compile(r\"frame=(\\d+)\")\n        for line in self.proc.stdout:\n            line = line.rstrip()\n            self._enqueue({\"type\": \"log\", \"line\": line})\n            self.last_output_time = time.time()\n            match = frame_re.search(line)\n            if match:\n                self.current_frame = int(match.group(1))\n                percent = self._progress_percent()\n                self._enqueue(\n                    {\n                        \"type\": \"progress\",\n                        \"frame\": self.current_frame,\n                        \"total\": self.total_frames,\n                        \"percent\": percent,\n                    }\n                )\n\n        ret = self.proc.wait()\n        with self.lock:\n            self.running = False\n            self.proc = None\n\n        if self.stop_requested:\n            self._enqueue({\"type\": \"status\", \"state\": \"已停止\"})\n        elif ret == 0:\n            self._enqueue({\"type\": \"status\", \"state\": \"完成\"})\n        else:\n            self._enqueue({\"type\": \"status\", \"state\": \"失败\"})\n\n    def _heartbeat(self) -> None:\n        while True:\n            time.sleep(2)\n            if not self.running:\n                return\n\n    def _progress_percent(self) -> float:\n        if not self.total_frames:\n            return -1.0\n        return min(100.0, (self.current_frame / max(1, self.total_frames)) * 100.0)\n\n    def _enqueue(self, msg: dict) -> None:\n        try:\n            self.queue.put_nowait(msg)\n        except queue.Full:\n            pass\n\n    def _build_command(self, payload: dict) -> list[str]:\n        cmd = [sys.executable, \"-u\", \"tools/video_to_sbs.py\"]\n        cmd += [\"--video\", payload[\"video\"]]\n        cmd += [\"--out\", payload[\"out\"]]\n\n        def add(flag: str, value) -> None:\n            if value is None:\n                return\n            if isinstance(value, str) and not value:\n                return\n            cmd.extend([flag, str(value)])\n\n        add(\"--debug_dir\", payload.get(\"debug_dir\"))\n        add(\"--baseline_m\", payload.get(\"baseline_m\"))\n        add(\"--baseline_min_m\", payload.get(\"baseline_min_m\"))\n        add(\"--max_disp_px\", payload.get(\"max_disp_px\"))\n        add(\"--fov_deg\", payload.get(\"fov_deg\"))\n        add(\"--cut_threshold\", payload.get(\"cut_threshold\"))\n        add(\"--min_shot_len\", payload.get(\"min_shot_len\"))\n        add(\"--max_shot_len\", payload.get(\"max_shot_len\"))\n        add(\"--min_inliers\", payload.get(\"min_inliers\"))\n        add(\"--max_reproj\", payload.get(\"max_reproj\"))\n        add(\"--ffmpeg_crf\", payload.get(\"ffmpeg_crf\"))\n        add(\"--ffmpeg_preset\", payload.get(\"ffmpeg_preset\"))\n        add(\"--inpaint_radius\", payload.get(\"inpaint_radius\"))\n        add(\"--debug_interval\", payload.get(\"debug_interval\"))\n        if payload.get(\"max_frames\"):\n            add(\"--max_frames\", payload.get(\"max_frames\"))\n        add(\"--log_path\", payload.get(\"log_path\"))\n        add(\"--device\", payload.get(\"device\"))\n        return cmd\n\n    def _resolve_output(self, out_value: str | None) -> str:\n        if not out_value:\n            out_value = f\"output_{int(time.time())}.mp4\"\n        out_path = Path(out_value)\n        if not out_path.suffix:\n            out_path = out_path.with_suffix(\".mp4\")\n        if out_path.is_absolute() or \":\" in out_value or out_value.startswith(\"\\\\\\\\\"):\n            return str(out_path)\n        return str(OUTPUTS_DIR / out_path.name)\n\n    @staticmethod\n    def _get_total_frames(video_path: Path) -> int:\n        cap = cv2.VideoCapture(str(video_path))\n        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n        cap.release()\n        return total\n\n\nrunner = Runner()\n\n\n@app.route(\"/\")\ndef index():\n    return send_from_directory(app.static_folder, \"index.html\")\n\n@app.route(\"/favicon.ico\")\ndef favicon():\n    return (\"\", 204)\n\n\n@app.route(\"/api/run\", methods=[\"POST\"])\ndef api_run():\n    payload = request.get_json(force=True, silent=True) or {}\n    return jsonify(runner.start(payload))\n\n\n@app.route(\"/api/stop\", methods=[\"POST\"])\ndef api_stop():\n    return jsonify(runner.stop())\n\n\n@app.route(\"/api/stream\")\ndef api_stream():\n    return Response(runner.stream(), mimetype=\"text/event-stream\")\n\n@app.route(\"/api/ping\")\ndef api_ping():\n    return jsonify({\"ok\": True})\n\n\ndef _pick_file(save: bool) -> tuple[str, str]:\n    LOGGER.info(\"File picker start. save=%s\", save)\n    ps_script = [\n        \"Add-Type -AssemblyName System.Windows.Forms;\",\n    ]\n    if save:\n        ps_script.append(\"$dlg = New-Object System.Windows.Forms.SaveFileDialog;\")\n        ps_script.append(\"$dlg.Filter = 'MP4 (*.mp4)|*.mp4|All files (*.*)|*.*';\")\n        ps_script.append(\"$dlg.DefaultExt = 'mp4';\")\n    else:\n        ps_script.append(\"$dlg = New-Object System.Windows.Forms.OpenFileDialog;\")\n        ps_script.append(\n            \"$dlg.Filter = 'Video (*.mp4;*.mov;*.mkv)|*.mp4;*.mov;*.mkv|All files (*.*)|*.*';\"\n        )\n        ps_script.append(\"$dlg.Multiselect = $false;\")\n    ps_script.append(\"$null = $dlg.ShowDialog();\")\n    ps_script.append(\"if ($dlg.FileName) { Write-Output $dlg.FileName }\")\n\n    try:\n        result = subprocess.run(\n            [\"powershell\", \"-NoProfile\", \"-Command\", \" \".join(ps_script)],\n            capture_output=True,\n            text=True,\n            check=False,\n        )\n        path = result.stdout.strip()\n        LOGGER.info(\n            \"PS picker exit=%s stdout=%s stderr=%s\",\n            result.returncode,\n            result.stdout.strip(),\n            result.stderr.strip(),\n        )\n        if path:\n            return path, \"\"\n    except Exception as exc:\n        LOGGER.exception(\"PS picker failed: %s\", exc)\n        return \"\", f\"PS picker failed: {exc}\"\n\n    script = [\n        \"import tkinter as tk\",\n        \"from tkinter import filedialog\",\n        \"root = tk.Tk()\",\n        \"root.withdraw()\",\n        \"root.attributes('-topmost', True)\",\n    ]\n    if save:\n        script.append(\n            \"path = filedialog.asksaveasfilename(\"\n            \"defaultextension='.mp4', \"\n            \"filetypes=[('MP4', '*.mp4'), ('All', '*.*')])\"\n        )\n    else:\n        script.append(\n            \"path = filedialog.askopenfilename(\"\n            \"filetypes=[('Video', '*.mp4;*.mov;*.mkv'), ('All', '*.*')])\"\n        )\n    script.append(\"root.destroy()\")\n    script.append(\"print(path or '')\")\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-c\", \";\".join(script)],\n            capture_output=True,\n            text=True,\n            check=False,\n        )\n        path = result.stdout.strip()\n        LOGGER.info(\n            \"Tk picker exit=%s stdout=%s stderr=%s\",\n            result.returncode,\n            result.stdout.strip(),\n            result.stderr.strip(),\n        )\n        if path:\n            return path, \"\"\n        return \"\", result.stderr.strip() or \"Tk picker returned empty path.\"\n    except Exception as exc:\n        LOGGER.exception(\"Tk picker failed: %s\", exc)\n        return \"\", f\"Tk picker failed: {exc}\"\n\n\n@app.route(\"/api/pick_input\", methods=[\"POST\"])\ndef api_pick_input():\n    path, error = _pick_file(save=False)\n    LOGGER.info(\"Pick input result ok=%s path=%s error=%s\", bool(path), path, error)\n    return jsonify({\"path\": path, \"ok\": bool(path), \"error\": error})\n\n\n@app.route(\"/api/pick_output\", methods=[\"POST\"])\ndef api_pick_output():\n    path, error = _pick_file(save=True)\n    LOGGER.info(\"Pick output result ok=%s path=%s error=%s\", bool(path), path, error)\n    return jsonify({\"path\": path, \"ok\": bool(path), \"error\": error})\n\n\n@app.route(\"/api/upload\", methods=[\"POST\"])\ndef api_upload():\n    if \"file\" not in request.files:\n        return jsonify({\"ok\": False, \"error\": \"未收到文件\"})\n    file = request.files[\"file\"]\n    if not file.filename:\n        return jsonify({\"ok\": False, \"error\": \"文件名为空\"})\n    name = safe_filename(file.filename)\n    stamp = int(time.time())\n    save_path = UPLOADS_DIR / f\"{stamp}_{name}\"\n    file.save(save_path)\n    suggested_output = Path(name).stem + \"_sbs.mp4\"\n    LOGGER.info(\"Uploaded file %s -> %s\", file.filename, save_path)\n    return jsonify(\n        {\n            \"ok\": True,\n            \"path\": str(save_path),\n            \"name\": file.filename,\n            \"suggested_output\": suggested_output,\n        }\n    )\n\n\n@app.route(\"/api/download\")\ndef api_download():\n    path = Path(runner.last_output_path) if runner.last_output_path else None\n    if path is None or not path.exists():\n        return (\"\", 404)\n    return send_file(path, as_attachment=True, download_name=path.name)\n\n\n@app.route(\"/<path:path>\")\ndef static_proxy(path: str):\n    return send_from_directory(app.static_folder, path)\n\n\n@app.after_request\ndef add_no_cache_headers(response):\n    response.headers[\"Cache-Control\"] = \"no-store, no-cache, must-revalidate, max-age=0\"\n    response.headers[\"Pragma\"] = \"no-cache\"\n    response.headers[\"Expires\"] = \"0\"\n    return response\n\n\ndef main() -> int:\n    app.run(host=\"127.0.0.1\", port=7860, debug=False, threaded=True)\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n"}, {"path": "tools/video_to_sbs.py", "content": "import argparse\nimport logging\nimport os\nimport sys\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nROOT_DIR = Path(__file__).resolve().parents[1]\nif str(ROOT_DIR) not in sys.path:\n    sys.path.insert(0, str(ROOT_DIR))\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ.setdefault(\"PYTHONUTF8\", \"1\")\nos.environ.setdefault(\"PYTHONIOENCODING\", \"utf-8\")\nos.environ.setdefault(\"TORCH_CUDA_ARCH_LIST\", \"12.0\")\n_tmp_root = Path(r\"C:\\temp\")\n_tmp_root.mkdir(parents=True, exist_ok=True)\nos.environ.setdefault(\"TMP\", str(_tmp_root))\nos.environ.setdefault(\"TEMP\", str(_tmp_root))\nos.environ.setdefault(\"TMPDIR\", str(_tmp_root))\nos.environ.setdefault(\"TORCH_EXTENSIONS_DIR\", str(_tmp_root / \"torch_extensions\"))\n_cuda_root = Path(r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v13.0\")\n_cuda_bin = _cuda_root / \"bin\"\nif _cuda_bin.exists():\n    os.environ.setdefault(\"CUDA_PATH\", str(_cuda_root))\n    os.environ.setdefault(\"CUDA_HOME\", str(_cuda_root))\n    os.environ.setdefault(\"CUDA_ROOT\", str(_cuda_root))\n    os.environ[\"PATH\"] = f\"{_cuda_bin};{os.environ.get('PATH', '')}\"\n    os.environ.setdefault(\"GSPLAT_CUDA_HOME\", str(_cuda_root))\n\n_msvc_root = Path(r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\")\nif _msvc_root.exists():\n    cl_candidates = list(_msvc_root.glob(\"**/bin/Hostx64/x64/cl.exe\"))\n    if cl_candidates:\n        cl_bin = cl_candidates[0].parent\n        os.environ[\"PATH\"] = f\"{cl_bin};{os.environ.get('PATH', '')}\"\n\nimport cv2\nimport numpy as np\nimport torch\nfrom torch.utils import cpp_extension\n\ncpp_extension.SUBPROCESS_DECODE_ARGS = (\"utf-8\", \"ignore\")\n\nfrom sbs.ffmpeg_writer import FFmpegWriter\nfrom sbs.rendering import (\n    build_intrinsics,\n    clamp_baseline,\n    compute_stereo_extrinsics,\n    inpaint_holes,\n)\nfrom sbs.sharp_backend import SharpPredictor, SharpRenderer, load_or_predict_ply\nfrom sbs.shots import ShotDetector\nfrom sbs.tracking import (\n    filter_fundamental,\n    select_keypoints,\n    smooth_pose,\n    solve_pnp,\n    track_keypoints,\n)\nfrom sbs.utils import (\n    Timer,\n    ensure_dir,\n    pad_even,\n    safe_filename,\n    save_depth_png,\n    save_json,\n    setup_logging,\n    video_cache_key,\n)\n\n\n@dataclass\nclass KeyframeState:\n    frame_idx: int\n    segment_id: int\n    gaussians: object\n    metadata: object\n    renderer: SharpRenderer\n    depth_map: np.ndarray\n    median_depth: float\n    intrinsics: np.ndarray\n    k_mat: np.ndarray\n    gray: np.ndarray\n    key_pts_2d: np.ndarray\n    key_pts_3d: np.ndarray\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"2D video to SBS 3D using SHARP keyframes\")\n    parser.add_argument(\"--video\", required=True, help=\"Path to input video\")\n    parser.add_argument(\"--out\", required=True, help=\"Path to output SBS mp4\")\n    parser.add_argument(\"--cache_dir\", default=\".cache_sharp\", help=\"Cache directory\")\n    parser.add_argument(\"--debug_dir\", default=None, help=\"Optional debug output dir\")\n    parser.add_argument(\"--log_path\", default=None, help=\"Optional log file path\")\n\n    parser.add_argument(\"--fov_deg\", type=float, default=60.0, help=\"Assumed horizontal FOV\")\n    parser.add_argument(\"--focal_px\", type=float, default=None, help=\"Override focal length in pixels\")\n\n    parser.add_argument(\"--baseline_m\", type=float, default=0.03, help=\"Stereo baseline in meters\")\n    parser.add_argument(\"--baseline_min_m\", type=float, default=0.0, help=\"Minimum baseline clamp\")\n    parser.add_argument(\"--max_disp_px\", type=float, default=30.0, help=\"Disparity clamp in pixels\")\n\n    parser.add_argument(\"--cut_threshold\", type=float, default=0.6, help=\"Histogram cut threshold\")\n    parser.add_argument(\"--min_shot_len\", type=float, default=0.5, help=\"Min shot length seconds\")\n    parser.add_argument(\"--max_shot_len\", type=float, default=5.0, help=\"Max shot length seconds\")\n\n    parser.add_argument(\"--max_features\", type=int, default=2000, help=\"Max keyframe features\")\n    parser.add_argument(\"--feature_quality\", type=float, default=0.01, help=\"GFTT quality level\")\n    parser.add_argument(\"--feature_min_dist\", type=int, default=7, help=\"GFTT min distance\")\n\n    parser.add_argument(\"--flow_fb_thresh\", type=float, default=1.0, help=\"LK forward-backward threshold\")\n    parser.add_argument(\"--fundamental_thresh\", type=float, default=1.0, help=\"F-matrix RANSAC threshold\")\n\n    parser.add_argument(\"--pnp_ransac_iters\", type=int, default=200, help=\"PnP RANSAC iterations\")\n    parser.add_argument(\"--pnp_reproj\", type=float, default=3.0, help=\"PnP reprojection error\")\n    parser.add_argument(\"--min_inliers\", type=int, default=80, help=\"PnP min inliers\")\n    parser.add_argument(\"--max_reproj\", type=float, default=2.5, help=\"PnP max reprojection error\")\n    parser.add_argument(\"--pose_smooth\", type=float, default=0.25, help=\"Pose smoothing alpha\")\n\n    parser.add_argument(\"--depth_min\", type=float, default=0.0, help=\"Min depth clamp\")\n    parser.add_argument(\"--depth_max\", type=float, default=0.0, help=\"Max depth clamp\")\n    parser.add_argument(\"--depth_q_low\", type=float, default=0.02, help=\"Depth quantile low\")\n    parser.add_argument(\"--depth_q_high\", type=float, default=0.98, help=\"Depth quantile high\")\n\n    parser.add_argument(\"--ffmpeg_crf\", type=int, default=18, help=\"FFmpeg CRF\")\n    parser.add_argument(\"--ffmpeg_preset\", default=\"slow\", help=\"FFmpeg preset\")\n\n    parser.add_argument(\"--inpaint_radius\", type=int, default=3, help=\"Inpaint radius for holes\")\n    parser.add_argument(\"--debug_interval\", type=int, default=30, help=\"Debug frame interval\")\n    parser.add_argument(\"--max_frames\", type=int, default=0, help=\"Limit frames for debug\")\n\n    parser.add_argument(\"--sharp_ckpt\", default=None, help=\"Path to SHARP checkpoint\")\n    parser.add_argument(\"--device\", default=\"cuda\", help=\"Torch device\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose logging\")\n    return parser.parse_args()\n\n\ndef compute_focal_px(width: int, fov_deg: float, focal_override: float | None) -> float:\n    if focal_override and focal_override > 0:\n        return focal_override\n    fov_rad = np.deg2rad(fov_deg)\n    return 0.5 * width / np.tan(0.5 * fov_rad)\n\n\ndef build_3d_points(\n    pts_2d: np.ndarray,\n    depth_map: np.ndarray,\n    k_mat: np.ndarray,\n    depth_min: float,\n    depth_max: float,\n) -> tuple[np.ndarray, np.ndarray]:\n    if len(pts_2d) == 0:\n        return np.empty((0, 2), dtype=np.float32), np.empty((0, 3), dtype=np.float32)\n\n    fx = k_mat[0, 0]\n    fy = k_mat[1, 1]\n    cx = k_mat[0, 2]\n    cy = k_mat[1, 2]\n\n    xs = np.clip(np.round(pts_2d[:, 0]).astype(int), 0, depth_map.shape[1] - 1)\n    ys = np.clip(np.round(pts_2d[:, 1]).astype(int), 0, depth_map.shape[0] - 1)\n    zs = depth_map[ys, xs]\n\n    valid = np.isfinite(zs) & (zs > 0)\n    if depth_min > 0:\n        valid &= zs >= depth_min\n    if depth_max > 0:\n        valid &= zs <= depth_max\n\n    pts_2d = pts_2d[valid]\n    zs = zs[valid]\n    if len(pts_2d) == 0:\n        return np.empty((0, 2), dtype=np.float32), np.empty((0, 3), dtype=np.float32)\n\n    xs = (pts_2d[:, 0] - cx) * zs / fx\n    ys = (pts_2d[:, 1] - cy) * zs / fy\n    pts_3d = np.stack([xs, ys, zs], axis=1).astype(np.float32)\n    return pts_2d.astype(np.float32), pts_3d\n\n\ndef compute_depth_range(depth_map: np.ndarray, q_low: float, q_high: float) -> tuple[float, float, float]:\n    valid = np.isfinite(depth_map) & (depth_map > 0)\n    if not np.any(valid):\n        return 0.0, 0.0, 0.0\n    depth_vals = depth_map[valid]\n    low = float(np.quantile(depth_vals, q_low))\n    high = float(np.quantile(depth_vals, q_high))\n    median = float(np.median(depth_vals))\n    return low, high, median\n\n\ndef create_keyframe(\n    frame_bgr: np.ndarray,\n    frame_idx: int,\n    segment_id: int,\n    predictor: SharpPredictor,\n    device: torch.device,\n    cache_dir: Path,\n    f_px: float,\n    args: argparse.Namespace,\n    debug_root: Path | None,\n) -> KeyframeState | None:\n    try:\n        timer = Timer()\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        ply_path = cache_dir / f\"seg{segment_id:03d}_f{frame_idx:06d}.ply\"\n        cached = ply_path.exists()\n        logging.info(\n            \"Keyframe build start: seg=%d frame=%d ply=%s\",\n            segment_id,\n            frame_idx,\n            \"cache\" if cached else \"predict\",\n        )\n\n        gaussians, metadata = load_or_predict_ply(predictor, frame_rgb, f_px, ply_path)\n        gaussians = gaussians.to(device)\n        renderer = SharpRenderer(metadata.color_space)\n\n        intrinsics = build_intrinsics(metadata.focal_length_px, metadata.resolution_px[0], metadata.resolution_px[1])\n        extrinsics = np.eye(4, dtype=np.float32)\n        _, depth, alpha = renderer.render(\n            gaussians,\n            intrinsics,\n            extrinsics,\n            metadata.resolution_px[0],\n            metadata.resolution_px[1],\n            device,\n        )\n\n        low, high, median = compute_depth_range(depth, args.depth_q_low, args.depth_q_high)\n        depth_min = max(low, args.depth_min) if low > 0 else args.depth_min\n        depth_max = min(high, args.depth_max) if args.depth_max > 0 and high > 0 else (high if high > 0 else args.depth_max)\n\n        gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n        key_pts_2d = select_keypoints(gray, args.max_features, args.feature_quality, args.feature_min_dist)\n        key_pts_2d, key_pts_3d = build_3d_points(key_pts_2d, depth, intrinsics[:3, :3], depth_min, depth_max)\n\n        if debug_root:\n            seg_dir = debug_root / f\"segment_{segment_id:03d}\"\n            ensure_dir(seg_dir)\n            cv2.imwrite(str(seg_dir / f\"keyframe_{frame_idx:06d}.png\"), frame_bgr)\n            save_depth_png(seg_dir / f\"keyframe_{frame_idx:06d}.depth.png\", depth)\n\n        logging.info(\n            \"Keyframe build done: seg=%d frame=%d pts=%d depth=%.3f ms=%.1f\",\n            segment_id,\n            frame_idx,\n            len(key_pts_2d),\n            median,\n            timer.elapsed_ms(),\n        )\n\n        return KeyframeState(\n            frame_idx=frame_idx,\n            segment_id=segment_id,\n            gaussians=gaussians,\n            metadata=metadata,\n            renderer=renderer,\n            depth_map=depth,\n            median_depth=median,\n            intrinsics=intrinsics,\n            k_mat=intrinsics[:3, :3],\n            gray=gray,\n            key_pts_2d=key_pts_2d,\n            key_pts_3d=key_pts_3d,\n        )\n    except Exception:\n        logging.exception(\"Keyframe creation failed at frame %d\", frame_idx)\n        return None\n\n\ndef main() -> int:\n    args = parse_args()\n    setup_logging(args.log_path, args.verbose)\n\n    if not torch.cuda.is_available():\n        logging.error(\"CUDA is required for rendering.\")\n        return 2\n\n    video_path = Path(args.video)\n    out_path = Path(args.out)\n\n    if not video_path.exists():\n        logging.error(\"Input video not found: %s\", video_path)\n        return 2\n\n    cap = cv2.VideoCapture(str(video_path))\n    if not cap.isOpened():\n        logging.error(\"Failed to open video: %s\", video_path)\n        return 2\n\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    width_even = width + (width % 2)\n    height_even = height + (height % 2)\n    logging.info(\n        \"Video opened: path=%s fps=%.2f size=%dx%d\",\n        video_path,\n        fps,\n        width,\n        height,\n    )\n\n    f_px = compute_focal_px(width_even, args.fov_deg, args.focal_px)\n\n    cache_root = Path(args.cache_dir) / safe_filename(video_path.stem) / video_cache_key(video_path)\n    ensure_dir(cache_root)\n\n    debug_root = Path(args.debug_dir) if args.debug_dir else None\n    if debug_root:\n        ensure_dir(debug_root)\n\n    if args.device != \"cuda\":\n        logging.warning(\"Force CUDA on RTX 5070 Ti. Ignoring --device=%s\", args.device)\n    torch.cuda.set_device(0)\n    device = torch.device(\"cuda:0\")\n    logging.info(\"Using CUDA device: %s\", torch.cuda.get_device_name(0))\n    logging.info(\"Loading SHARP predictor...\")\n    predictor = SharpPredictor(device=str(device), checkpoint_path=args.sharp_ckpt)\n    logging.info(\"SHARP predictor ready.\")\n\n    shot_detector = ShotDetector(\n        cut_threshold=args.cut_threshold,\n        min_len_frames=max(1, int(args.min_shot_len * fps)),\n        max_len_frames=max(1, int(args.max_shot_len * fps)),\n    )\n\n    writer = None\n    keyframe: KeyframeState | None = None\n    prev_rvec = None\n    prev_tvec = None\n    frame_idx = 0\n    segment_id = 0\n    pose_log = []\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            frame, (width_even, height_even) = pad_even(frame)\n            if writer is None:\n                writer = FFmpegWriter(\n                    out_path,\n                    width_even * 2,\n                    height_even,\n                    fps,\n                    args.ffmpeg_crf,\n                    args.ffmpeg_preset,\n                )\n\n            cut = shot_detector.update(frame)\n            need_keyframe = keyframe is None or cut\n\n            new_keyframe_reason = None\n            if need_keyframe:\n                new_keyframe_reason = \"init\" if keyframe is None else \"cut\"\n                keyframe = create_keyframe(\n                    frame,\n                    frame_idx,\n                    segment_id,\n                    predictor,\n                    device,\n                    cache_root,\n                    f_px,\n                    args,\n                    debug_root,\n                )\n                if keyframe is None:\n                    shot_detector.reset()\n                else:\n                    segment_id += 1\n                    shot_detector.reset()\n                    prev_rvec = None\n                    prev_tvec = None\n                    logging.info(\n                        \"Keyframe created: seg=%d frame=%d reason=%s pts=%d\",\n                        keyframe.segment_id,\n                        keyframe.frame_idx,\n                        new_keyframe_reason,\n                        len(keyframe.key_pts_2d),\n                    )\n\n            if keyframe is None:\n                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                sbs = np.concatenate([frame_rgb, frame_rgb], axis=1)\n                writer.write(sbs.tobytes())\n                frame_idx += 1\n                if args.max_frames and frame_idx >= args.max_frames:\n                    break\n                continue\n\n            tracking_timer = Timer()\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            if new_keyframe_reason is None:\n                prev_pts, curr_pts, indices = track_keypoints(\n                    keyframe.gray,\n                    gray,\n                    keyframe.key_pts_2d,\n                    args.flow_fb_thresh,\n                )\n                prev_pts, curr_pts, indices = filter_fundamental(\n                    prev_pts,\n                    curr_pts,\n                    indices,\n                    args.fundamental_thresh,\n                )\n                obj_pts = keyframe.key_pts_3d[indices]\n                pnp_result = solve_pnp(\n                    obj_pts,\n                    curr_pts,\n                    keyframe.k_mat,\n                    args.pnp_ransac_iters,\n                    args.pnp_reproj,\n                )\n\n                if (\n                    (not pnp_result.success)\n                    or (pnp_result.inliers is None)\n                    or (len(pnp_result.inliers) < args.min_inliers)\n                    or (pnp_result.reproj_error > args.max_reproj)\n                ):\n                    logging.warning(\n                        \"PnP failed: inliers=%s reproj=%.3f -> new keyframe\",\n                        0 if pnp_result.inliers is None else len(pnp_result.inliers),\n                        pnp_result.reproj_error,\n                    )\n                    new_keyframe_reason = \"pnp_fail\"\n                    keyframe = create_keyframe(\n                        frame,\n                        frame_idx,\n                        segment_id,\n                        predictor,\n                        device,\n                        cache_root,\n                        f_px,\n                        args,\n                        debug_root,\n                    )\n                    if keyframe is None:\n                        shot_detector.reset()\n                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                        sbs = np.concatenate([frame_rgb, frame_rgb], axis=1)\n                        writer.write(sbs.tobytes())\n                        frame_idx += 1\n                        if args.max_frames and frame_idx >= args.max_frames:\n                            break\n                        continue\n                    segment_id += 1\n                    shot_detector.reset()\n                    prev_rvec = None\n                    prev_tvec = None\n                    logging.info(\n                        \"Keyframe created: seg=%d frame=%d reason=%s pts=%d\",\n                        keyframe.segment_id,\n                        keyframe.frame_idx,\n                        new_keyframe_reason,\n                        len(keyframe.key_pts_2d),\n                    )\n                    rvec = np.zeros((3, 1), dtype=np.float32)\n                    tvec = np.zeros((3, 1), dtype=np.float32)\n                    pnp_result = None\n                else:\n                    rvec, tvec = pnp_result.rvec, pnp_result.tvec\n                    if prev_rvec is not None and prev_tvec is not None:\n                        rvec, tvec = smooth_pose(prev_rvec, prev_tvec, rvec, tvec, args.pose_smooth)\n\n                    prev_rvec, prev_tvec = rvec, tvec\n            else:\n                rvec = np.zeros((3, 1), dtype=np.float32)\n                tvec = np.zeros((3, 1), dtype=np.float32)\n                pnp_result = None\n\n            rmat, _ = cv2.Rodrigues(rvec)\n            baseline = clamp_baseline(\n                args.baseline_m,\n                args.max_disp_px,\n                keyframe.median_depth,\n                keyframe.k_mat[0, 0],\n                args.baseline_min_m,\n            )\n\n            render_timer = Timer()\n            left_extr, right_extr = compute_stereo_extrinsics(rmat, tvec, baseline)\n            left_color, left_depth, left_alpha = keyframe.renderer.render(\n                keyframe.gaussians,\n                keyframe.intrinsics,\n                left_extr,\n                keyframe.metadata.resolution_px[0],\n                keyframe.metadata.resolution_px[1],\n                device,\n            )\n            right_color, right_depth, right_alpha = keyframe.renderer.render(\n                keyframe.gaussians,\n                keyframe.intrinsics,\n                right_extr,\n                keyframe.metadata.resolution_px[0],\n                keyframe.metadata.resolution_px[1],\n                device,\n            )\n\n            left_img = np.clip(left_color * 255.0, 0, 255).astype(np.uint8)\n            right_img = np.clip(right_color * 255.0, 0, 255).astype(np.uint8)\n\n            left_img = inpaint_holes(left_img, left_alpha, args.inpaint_radius)\n            right_img = inpaint_holes(right_img, right_alpha, args.inpaint_radius)\n\n            sbs = np.concatenate([left_img, right_img], axis=1)\n            writer.write(sbs.tobytes())\n\n            if debug_root and frame_idx % args.debug_interval == 0:\n                seg_dir = debug_root / f\"segment_{keyframe.segment_id:03d}\"\n                ensure_dir(seg_dir)\n                cv2.imwrite(str(seg_dir / f\"sbs_{frame_idx:06d}.jpg\"), cv2.cvtColor(sbs, cv2.COLOR_RGB2BGR))\n                if new_keyframe_reason is None:\n                    save_depth_png(seg_dir / f\"depth_{frame_idx:06d}.png\", left_depth)\n\n            tracking_ms = tracking_timer.elapsed_ms()\n            render_ms = render_timer.elapsed_ms()\n            inliers = 0\n            reproj = 0.0\n            if pnp_result is not None and pnp_result.inliers is not None:\n                inliers = len(pnp_result.inliers)\n                reproj = pnp_result.reproj_error\n\n            logging.info(\n                \"seg=%d frame=%d inliers=%d reproj=%.3f new_keyframe=%s pnp_ms=%.2f render_ms=%.2f baseline=%.4f\",\n                keyframe.segment_id,\n                frame_idx,\n                inliers,\n                reproj,\n                \"yes\" if new_keyframe_reason else \"no\",\n                tracking_ms,\n                render_ms,\n                baseline,\n            )\n\n            pose_log.append(\n                {\n                    \"frame\": frame_idx,\n                    \"segment\": keyframe.segment_id,\n                    \"inliers\": inliers,\n                    \"reproj\": reproj,\n                    \"baseline\": baseline,\n                    \"new_keyframe\": bool(new_keyframe_reason),\n                    \"pnp_ms\": tracking_ms,\n                    \"render_ms\": render_ms,\n                }\n            )\n            frame_idx += 1\n            if args.max_frames and frame_idx >= args.max_frames:\n                break\n\n        if debug_root:\n            save_json(debug_root / \"pose_log.json\", {\"items\": pose_log})\n\n    finally:\n        cap.release()\n        if writer is not None:\n            writer.close()\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}, {"path": "tools/env_check.py", "content": "import importlib\nimport shutil\nimport subprocess\nimport sys\n\n\ndef check_import(name: str) -> bool:\n    try:\n        importlib.import_module(name)\n        return True\n    except Exception:\n        return False\n\n\ndef check_ffmpeg() -> bool:\n    exe = shutil.which(\"ffmpeg\")\n    if not exe:\n        return False\n    try:\n        subprocess.run([exe, \"-version\"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        return True\n    except Exception:\n        return False\n\n\ndef main() -> int:\n    ok = True\n\n    print(\"Python:\", sys.version.split()[0])\n\n    torch_ok = check_import(\"torch\")\n    print(\"torch:\", \"OK\" if torch_ok else \"MISSING\")\n    if torch_ok:\n        import torch\n\n        print(\"torch.cuda:\", \"OK\" if torch.cuda.is_available() else \"MISSING\")\n        print(\"torch.cuda.version:\", torch.version.cuda)\n        if not torch.cuda.is_available():\n            ok = False\n    else:\n        ok = False\n\n    cv2_ok = check_import(\"cv2\")\n    print(\"opencv:\", \"OK\" if cv2_ok else \"MISSING\")\n    ok = ok and cv2_ok\n\n    numpy_ok = check_import(\"numpy\")\n    print(\"numpy:\", \"OK\" if numpy_ok else \"MISSING\")\n    ok = ok and numpy_ok\n\n    sharp_ok = check_import(\"sharp\")\n    print(\"sharp:\", \"OK\" if sharp_ok else \"MISSING\")\n    ok = ok and sharp_ok\n\n    gsplat_ok = check_import(\"gsplat\")\n    print(\"gsplat:\", \"OK\" if gsplat_ok else \"MISSING\")\n    ok = ok and gsplat_ok\n\n    flask_ok = check_import(\"flask\")\n    print(\"flask:\", \"OK\" if flask_ok else \"MISSING\")\n    ok = ok and flask_ok\n\n    ffmpeg_ok = check_ffmpeg()\n    print(\"ffmpeg:\", \"OK\" if ffmpeg_ok else \"MISSING\")\n    ok = ok and ffmpeg_ok\n\n    return 0 if ok else 2\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}, {"path": "sbs/__init__.py", "content": ""}, {"path": "sbs/utils.py", "content": "import hashlib\nimport json\nimport logging\nimport os\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\n\n\nclass Timer:\n    def __init__(self) -> None:\n        self.start = time.perf_counter()\n\n    def elapsed_ms(self) -> float:\n        return (time.perf_counter() - self.start) * 1000.0\n\n\ndef ensure_dir(path: Path) -> None:\n    path.mkdir(parents=True, exist_ok=True)\n\n\ndef setup_logging(log_path: str | None, verbose: bool) -> None:\n    level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(\n        level=level,\n        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n    )\n    if log_path:\n        file_handler = logging.FileHandler(log_path, encoding=\"utf-8\")\n        file_handler.setLevel(level)\n        file_handler.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n        logging.getLogger().addHandler(file_handler)\n\n\ndef pad_even(frame: np.ndarray) -> tuple[np.ndarray, tuple[int, int]]:\n    height, width = frame.shape[:2]\n    pad_w = width % 2\n    pad_h = height % 2\n    if pad_w == 0 and pad_h == 0:\n        return frame, (width, height)\n    padded = cv2.copyMakeBorder(frame, 0, pad_h, 0, pad_w, cv2.BORDER_CONSTANT, value=0)\n    return padded, (width + pad_w, height + pad_h)\n\n\ndef compute_hist_hsv(frame_bgr: np.ndarray) -> np.ndarray:\n    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n    hist = cv2.calcHist([hsv], [0, 1], None, [16, 16], [0, 180, 0, 256])\n    cv2.normalize(hist, hist)\n    return hist\n\n\ndef hist_correlation(h1: np.ndarray, h2: np.ndarray) -> float:\n    return float(cv2.compareHist(h1, h2, cv2.HISTCMP_CORREL))\n\n\ndef save_json(path: Path, data: dict) -> None:\n    ensure_dir(path.parent)\n    path.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n\n\ndef save_depth_png(path: Path, depth: np.ndarray) -> None:\n    valid = np.isfinite(depth) & (depth > 0)\n    if not np.any(valid):\n        return\n    depth_vis = depth.copy()\n    depth_vis[~valid] = 0\n    min_val = float(np.percentile(depth_vis[valid], 1))\n    max_val = float(np.percentile(depth_vis[valid], 99))\n    if max_val <= min_val:\n        max_val = min_val + 1e-3\n    norm = (depth_vis - min_val) / (max_val - min_val)\n    norm = np.clip(norm, 0, 1)\n    img = (norm * 255).astype(np.uint8)\n    img = cv2.applyColorMap(img, cv2.COLORMAP_TURBO)\n    ensure_dir(path.parent)\n    cv2.imwrite(str(path), img)\n\n\ndef write_text(path: Path, text: str) -> None:\n    ensure_dir(path.parent)\n    path.write_text(text, encoding=\"utf-8\")\n\n\ndef safe_filename(name: str) -> str:\n    return \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in name)\n\n\ndef video_cache_key(video_path: Path) -> str:\n    stat = video_path.stat()\n    payload = f\"{video_path.resolve()}|{stat.st_size}|{stat.st_mtime_ns}\"\n    return hashlib.md5(payload.encode(\"utf-8\")).hexdigest()\n\n\ndef set_env_determinism() -> None:\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n    os.environ.setdefault(\"PYTHONHASHSEED\", \"0\")\n"}, {"path": "sbs/tracking.py", "content": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Tuple\n\nimport cv2\nimport numpy as np\n\n\n@dataclass\nclass PnPResult:\n    success: bool\n    rvec: np.ndarray\n    tvec: np.ndarray\n    inliers: np.ndarray | None\n    reproj_error: float\n\n\ndef select_keypoints(\n    gray: np.ndarray,\n    max_features: int,\n    quality: float,\n    min_distance: int,\n) -> np.ndarray:\n    pts = cv2.goodFeaturesToTrack(\n        gray,\n        maxCorners=max_features,\n        qualityLevel=quality,\n        minDistance=min_distance,\n        blockSize=7,\n        useHarrisDetector=False,\n    )\n    if pts is None:\n        return np.empty((0, 2), dtype=np.float32)\n    return pts.reshape(-1, 2).astype(np.float32)\n\n\ndef track_keypoints(\n    prev_gray: np.ndarray,\n    curr_gray: np.ndarray,\n    prev_pts: np.ndarray,\n    fb_thresh: float,\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    if len(prev_pts) == 0:\n        return prev_pts, prev_pts, np.array([], dtype=np.int32)\n    prev_pts_in = prev_pts.reshape(-1, 1, 2)\n    curr_pts, st, _ = cv2.calcOpticalFlowPyrLK(\n        prev_gray,\n        curr_gray,\n        prev_pts_in,\n        None,\n        winSize=(21, 21),\n        maxLevel=3,\n        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01),\n    )\n    back_pts, st_back, _ = cv2.calcOpticalFlowPyrLK(\n        curr_gray,\n        prev_gray,\n        curr_pts,\n        None,\n        winSize=(21, 21),\n        maxLevel=3,\n        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01),\n    )\n    if curr_pts is None or back_pts is None:\n        return (\n            np.empty((0, 2), dtype=np.float32),\n            np.empty((0, 2), dtype=np.float32),\n            np.array([], dtype=np.int32),\n        )\n\n    curr_pts = curr_pts.reshape(-1, 2)\n    back_pts = back_pts.reshape(-1, 2)\n    st = st.reshape(-1)\n    st_back = st_back.reshape(-1)\n\n    fb_err = np.linalg.norm(prev_pts - back_pts, axis=1)\n    mask = (st == 1) & (st_back == 1) & (fb_err < fb_thresh)\n    idx = np.arange(len(prev_pts), dtype=np.int32)[mask]\n    return prev_pts[mask], curr_pts[mask], idx\n\n\ndef filter_fundamental(\n    prev_pts: np.ndarray,\n    curr_pts: np.ndarray,\n    indices: np.ndarray,\n    ransac_thresh: float,\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    if len(prev_pts) < 8:\n        return prev_pts, curr_pts, indices\n    fmat, mask = cv2.findFundamentalMat(\n        prev_pts,\n        curr_pts,\n        cv2.FM_RANSAC,\n        ransac_thresh,\n        0.99,\n    )\n    if mask is None:\n        return prev_pts, curr_pts, indices\n    mask = mask.ravel().astype(bool)\n    return prev_pts[mask], curr_pts[mask], indices[mask]\n\n\ndef solve_pnp(\n    obj_pts: np.ndarray,\n    img_pts: np.ndarray,\n    k_mat: np.ndarray,\n    ransac_iters: int,\n    reproj_error: float,\n) -> PnPResult:\n    if len(obj_pts) < 6:\n        return PnPResult(False, np.zeros((3, 1)), np.zeros((3, 1)), None, float(\"inf\"))\n\n    success, rvec, tvec, inliers = cv2.solvePnPRansac(\n        obj_pts,\n        img_pts,\n        k_mat,\n        None,\n        iterationsCount=ransac_iters,\n        reprojectionError=reproj_error,\n        confidence=0.999,\n        flags=cv2.SOLVEPNP_EPNP,\n    )\n    if not success:\n        return PnPResult(False, np.zeros((3, 1)), np.zeros((3, 1)), None, float(\"inf\"))\n\n    if inliers is not None and len(inliers) >= 6:\n        inlier_obj = obj_pts[inliers.ravel()]\n        inlier_img = img_pts[inliers.ravel()]\n        success_refine, rvec, tvec = cv2.solvePnP(\n            inlier_obj,\n            inlier_img,\n            k_mat,\n            None,\n            rvec,\n            tvec,\n            useExtrinsicGuess=True,\n            flags=cv2.SOLVEPNP_ITERATIVE,\n        )\n        if not success_refine:\n            return PnPResult(False, rvec, tvec, inliers, float(\"inf\"))\n\n    reproj = compute_reprojection_error(obj_pts, img_pts, k_mat, rvec, tvec, inliers)\n    return PnPResult(True, rvec, tvec, inliers, reproj)\n\n\ndef compute_reprojection_error(\n    obj_pts: np.ndarray,\n    img_pts: np.ndarray,\n    k_mat: np.ndarray,\n    rvec: np.ndarray,\n    tvec: np.ndarray,\n    inliers: np.ndarray | None,\n) -> float:\n    if inliers is not None:\n        obj_pts = obj_pts[inliers.ravel()]\n        img_pts = img_pts[inliers.ravel()]\n    if len(obj_pts) == 0:\n        return float(\"inf\")\n    proj, _ = cv2.projectPoints(obj_pts, rvec, tvec, k_mat, None)\n    proj = proj.reshape(-1, 2)\n    err = np.linalg.norm(proj - img_pts, axis=1)\n    return float(np.mean(err))\n\n\ndef smooth_pose(\n    prev_rvec: np.ndarray,\n    prev_tvec: np.ndarray,\n    rvec: np.ndarray,\n    tvec: np.ndarray,\n    alpha: float,\n) -> Tuple[np.ndarray, np.ndarray]:\n    if prev_rvec is None or prev_tvec is None:\n        return rvec, tvec\n\n    r_prev = cv2.Rodrigues(prev_rvec)[0]\n    r_curr = cv2.Rodrigues(rvec)[0]\n\n    q_prev = rotmat_to_quat(r_prev)\n    q_curr = rotmat_to_quat(r_curr)\n    q_smooth = slerp(q_prev, q_curr, alpha)\n\n    r_smooth = quat_to_rotmat(q_smooth)\n    rvec_smooth, _ = cv2.Rodrigues(r_smooth)\n    t_smooth = (1.0 - alpha) * prev_tvec + alpha * tvec\n    return rvec_smooth, t_smooth\n\n\ndef rotmat_to_quat(rmat: np.ndarray) -> np.ndarray:\n    trace = np.trace(rmat)\n    if trace > 0:\n        s = 0.5 / np.sqrt(trace + 1.0)\n        w = 0.25 / s\n        x = (rmat[2, 1] - rmat[1, 2]) * s\n        y = (rmat[0, 2] - rmat[2, 0]) * s\n        z = (rmat[1, 0] - rmat[0, 1]) * s\n    else:\n        if rmat[0, 0] > rmat[1, 1] and rmat[0, 0] > rmat[2, 2]:\n            s = 2.0 * np.sqrt(1.0 + rmat[0, 0] - rmat[1, 1] - rmat[2, 2])\n            w = (rmat[2, 1] - rmat[1, 2]) / s\n            x = 0.25 * s\n            y = (rmat[0, 1] + rmat[1, 0]) / s\n            z = (rmat[0, 2] + rmat[2, 0]) / s\n        elif rmat[1, 1] > rmat[2, 2]:\n            s = 2.0 * np.sqrt(1.0 + rmat[1, 1] - rmat[0, 0] - rmat[2, 2])\n            w = (rmat[0, 2] - rmat[2, 0]) / s\n            x = (rmat[0, 1] + rmat[1, 0]) / s\n            y = 0.25 * s\n            z = (rmat[1, 2] + rmat[2, 1]) / s\n        else:\n            s = 2.0 * np.sqrt(1.0 + rmat[2, 2] - rmat[0, 0] - rmat[1, 1])\n            w = (rmat[1, 0] - rmat[0, 1]) / s\n            x = (rmat[0, 2] + rmat[2, 0]) / s\n            y = (rmat[1, 2] + rmat[2, 1]) / s\n            z = 0.25 * s\n    quat = np.array([w, x, y, z], dtype=np.float32)\n    return quat / np.linalg.norm(quat)\n\n\ndef quat_to_rotmat(q: np.ndarray) -> np.ndarray:\n    w, x, y, z = q\n    xx = x * x\n    yy = y * y\n    zz = z * z\n    xy = x * y\n    xz = x * z\n    yz = y * z\n    wx = w * x\n    wy = w * y\n    wz = w * z\n\n    return np.array(\n        [\n            [1 - 2 * (yy + zz), 2 * (xy - wz), 2 * (xz + wy)],\n            [2 * (xy + wz), 1 - 2 * (xx + zz), 2 * (yz - wx)],\n            [2 * (xz - wy), 2 * (yz + wx), 1 - 2 * (xx + yy)],\n        ],\n        dtype=np.float32,\n    )\n\n\ndef slerp(q0: np.ndarray, q1: np.ndarray, t: float) -> np.ndarray:\n    dot = float(np.dot(q0, q1))\n    if dot < 0.0:\n        q1 = -q1\n        dot = -dot\n    dot = min(max(dot, -1.0), 1.0)\n\n    if dot > 0.9995:\n        result = q0 + t * (q1 - q0)\n        return result / np.linalg.norm(result)\n\n    theta_0 = np.arccos(dot)\n    sin_theta_0 = np.sin(theta_0)\n    theta = theta_0 * t\n    sin_theta = np.sin(theta)\n\n    s0 = np.cos(theta) - dot * sin_theta / sin_theta_0\n    s1 = sin_theta / sin_theta_0\n    result = (s0 * q0) + (s1 * q1)\n    return result / np.linalg.norm(result)\n"}, {"path": "sbs/shots.py", "content": "from __future__ import annotations\n\nfrom dataclasses import dataclass\n\nimport numpy as np\n\nfrom .utils import compute_hist_hsv, hist_correlation\n\n\n@dataclass\nclass ShotDetector:\n    cut_threshold: float\n    min_len_frames: int\n    max_len_frames: int\n\n    last_hist: np.ndarray | None = None\n    frames_since_cut: int = 0\n\n    def update(self, frame_bgr: np.ndarray) -> bool:\n        self.frames_since_cut += 1\n        if self.last_hist is None:\n            self.last_hist = compute_hist_hsv(frame_bgr)\n            return False\n\n        hist = compute_hist_hsv(frame_bgr)\n        corr = hist_correlation(self.last_hist, hist)\n        self.last_hist = hist\n\n        if self.frames_since_cut < self.min_len_frames:\n            return False\n        if self.frames_since_cut >= self.max_len_frames:\n            self.frames_since_cut = 0\n            return True\n        if corr < self.cut_threshold:\n            self.frames_since_cut = 0\n            return True\n        return False\n\n    def reset(self) -> None:\n        self.last_hist = None\n        self.frames_since_cut = 0"}, {"path": "sbs/sharp_backend.py", "content": "from __future__ import annotations\n\nimport logging\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport requests\n\nfrom sharp.cli.predict import DEFAULT_MODEL_URL, predict_image\nfrom sharp.models import PredictorParams, create_predictor\nfrom sharp.utils.gaussians import Gaussians3D, SceneMetaData, load_ply, save_ply\nfrom sharp.utils.gsplat import GSplatRenderer\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass SharpPredictor:\n    def __init__(self, device: str, checkpoint_path: str | None) -> None:\n        self.device = torch.device(device)\n        self.model = create_predictor(PredictorParams())\n        self._load_weights(checkpoint_path)\n        self.model.eval()\n        self.model.to(self.device)\n\n    def _load_weights(self, checkpoint_path: str | None) -> None:\n        if checkpoint_path:\n            LOGGER.info(\"Loading SHARP checkpoint from %s\", checkpoint_path)\n            try:\n                state_dict = torch.load(checkpoint_path, weights_only=True)\n            except TypeError:\n                state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n        else:\n            LOGGER.info(\"Downloading SHARP checkpoint from %s\", DEFAULT_MODEL_URL)\n            try:\n                state_dict = torch.hub.load_state_dict_from_url(DEFAULT_MODEL_URL, progress=True)\n            except Exception as exc:\n                LOGGER.warning(\"Default download failed: %s\", exc)\n                cached_path = _get_checkpoint_cache_path(DEFAULT_MODEL_URL)\n                if not cached_path.exists():\n                    _download_checkpoint_insecure(DEFAULT_MODEL_URL, cached_path)\n                try:\n                    state_dict = torch.load(cached_path, weights_only=True)\n                except TypeError:\n                    state_dict = torch.load(cached_path, map_location=\"cpu\")\n        self.model.load_state_dict(state_dict)\n\n    @torch.no_grad()\n    def predict_gaussians(\n        self,\n        image_rgb: np.ndarray,\n        f_px: float,\n    ) -> Gaussians3D:\n        return predict_image(self.model, image_rgb, f_px, self.device)\n\n\nclass SharpRenderer:\n    def __init__(self, color_space: str) -> None:\n        self.renderer = GSplatRenderer(color_space=color_space)\n\n    @torch.no_grad()\n    def render(\n        self,\n        gaussians: Gaussians3D,\n        intrinsics: np.ndarray,\n        extrinsics: np.ndarray,\n        width: int,\n        height: int,\n        device: torch.device,\n    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n        intr = torch.from_numpy(intrinsics).to(device=device, dtype=torch.float32).unsqueeze(0)\n        extr = torch.from_numpy(extrinsics).to(device=device, dtype=torch.float32).unsqueeze(0)\n        outputs = self.renderer(\n            gaussians,\n            extrinsics=extr,\n            intrinsics=intr,\n            image_width=width,\n            image_height=height,\n        )\n        color = outputs.color[0].permute(1, 2, 0).detach().cpu().numpy()\n        depth = outputs.depth[0].squeeze(0).detach().cpu().numpy()\n        alpha = outputs.alpha[0].squeeze(0).detach().cpu().numpy()\n        return color, depth, alpha\n\n\ndef load_or_predict_ply(\n    predictor: SharpPredictor,\n    image_rgb: np.ndarray,\n    f_px: float,\n    ply_path: Path,\n) -> tuple[Gaussians3D, SceneMetaData]:\n    if ply_path.exists():\n        gaussians, metadata = load_ply(ply_path)\n        return gaussians, metadata\n\n    gaussians = predictor.predict_gaussians(image_rgb, f_px)\n    save_ply(gaussians, f_px, (image_rgb.shape[0], image_rgb.shape[1]), ply_path)\n    metadata = SceneMetaData(f_px, (image_rgb.shape[1], image_rgb.shape[0]), \"linearRGB\")\n    return gaussians, metadata\n\n\ndef _get_checkpoint_cache_path(url: str) -> Path:\n    filename = os.path.basename(url)\n    hub_dir = Path(torch.hub.get_dir())\n    cache_dir = hub_dir / \"checkpoints\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir / filename\n\n\ndef _download_checkpoint_insecure(url: str, dst: Path) -> None:\n    LOGGER.warning(\"Downloading checkpoint with SSL verification disabled.\")\n    tmp_path = dst.with_suffix(dst.suffix + \".tmp\")\n    with requests.get(url, stream=True, verify=False, timeout=60) as resp:\n        resp.raise_for_status()\n        with tmp_path.open(\"wb\") as f:\n            for chunk in resp.iter_content(chunk_size=1024 * 1024):\n                if chunk:\n                    f.write(chunk)\n    tmp_path.replace(dst)\n"}, {"path": "sbs/rendering.py", "content": "from __future__ import annotations\n\nimport numpy as np\n\n\ndef build_intrinsics(f_px: float, width: int, height: int) -> np.ndarray:\n    return np.array(\n        [\n            [f_px, 0.0, (width - 1) / 2.0, 0.0],\n            [0.0, f_px, (height - 1) / 2.0, 0.0],\n            [0.0, 0.0, 1.0, 0.0],\n            [0.0, 0.0, 0.0, 1.0],\n        ],\n        dtype=np.float32,\n    )\n\n\ndef pose_to_extrinsics(rmat: np.ndarray, tvec: np.ndarray) -> np.ndarray:\n    extrinsics = np.eye(4, dtype=np.float32)\n    extrinsics[:3, :3] = rmat\n    extrinsics[:3, 3] = tvec.reshape(3)\n    return extrinsics\n\n\ndef compute_stereo_extrinsics(\n    rmat: np.ndarray,\n    tvec: np.ndarray,\n    baseline_m: float,\n) -> tuple[np.ndarray, np.ndarray]:\n    cam_center = -rmat.T @ tvec.reshape(3)\n    right_world = rmat.T @ np.array([1.0, 0.0, 0.0], dtype=np.float32)\n\n    half = baseline_m / 2.0\n    left_center = cam_center - right_world * half\n    right_center = cam_center + right_world * half\n\n    t_left = -rmat @ left_center\n    t_right = -rmat @ right_center\n\n    left_extr = pose_to_extrinsics(rmat, t_left)\n    right_extr = pose_to_extrinsics(rmat, t_right)\n    return left_extr, right_extr\n\n\ndef clamp_baseline(\n    baseline_m: float,\n    max_disp_px: float,\n    median_depth: float,\n    f_px: float,\n    min_baseline_m: float,\n) -> float:\n    if median_depth <= 0:\n        return baseline_m\n    max_baseline = max_disp_px * median_depth / max(f_px, 1e-6)\n    baseline = min(baseline_m, max_baseline)\n    return max(baseline, min_baseline_m)\n\n\ndef inpaint_holes(color: np.ndarray, alpha: np.ndarray, radius: int) -> np.ndarray:\n    if radius <= 0:\n        return color\n    if alpha.ndim == 3:\n        alpha = alpha[:, :, 0]\n    mask = (alpha < 0.5).astype(np.uint8) * 255\n    if mask.sum() == 0:\n        return color\n    import cv2\n\n    return cv2.inpaint(color, mask, radius, cv2.INPAINT_TELEA)"}, {"path": "sbs/ffmpeg_writer.py", "content": "import logging\nimport subprocess\nfrom pathlib import Path\n\n\nclass FFmpegWriter:\n    def __init__(\n        self,\n        output_path: Path,\n        width: int,\n        height: int,\n        fps: float,\n        crf: int,\n        preset: str,\n    ) -> None:\n        self.output_path = output_path\n        self.width = width\n        self.height = height\n        self.fps = fps\n        self.crf = crf\n        self.preset = preset\n        self.proc = self._start()\n\n    def _start(self) -> subprocess.Popen:\n        self.output_path.parent.mkdir(parents=True, exist_ok=True)\n        cmd = [\n            \"ffmpeg\",\n            \"-y\",\n            \"-loglevel\",\n            \"error\",\n            \"-f\",\n            \"rawvideo\",\n            \"-pix_fmt\",\n            \"rgb24\",\n            \"-s\",\n            f\"{self.width}x{self.height}\",\n            \"-r\",\n            f\"{self.fps}\",\n            \"-i\",\n            \"-\",\n            \"-an\",\n            \"-c:v\",\n            \"libx264\",\n            \"-pix_fmt\",\n            \"yuv420p\",\n            \"-preset\",\n            self.preset,\n            \"-crf\",\n            str(self.crf),\n            \"-movflags\",\n            \"+faststart\",\n            str(self.output_path),\n        ]\n        logging.info(\"FFmpeg: %s\", \" \".join(cmd))\n        return subprocess.Popen(cmd, stdin=subprocess.PIPE)\n\n    def write(self, frame_rgb: bytes) -> None:\n        if self.proc.stdin is None:\n            raise RuntimeError(\"FFmpeg stdin is closed\")\n        self.proc.stdin.write(frame_rgb)\n\n    def close(self) -> None:\n        if self.proc.stdin is not None:\n            self.proc.stdin.close()\n        ret = self.proc.wait()\n        if ret != 0:\n            raise RuntimeError(f\"FFmpeg exited with code {ret}\")"}, {"path": "web/index.html", "content": "<!doctype html>\n<html lang=\"zh-CN\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>2D→3D SBS 生成台</title>\n    <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\" />\n    <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin />\n    <link\n      href=\"https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&display=swap\"\n      rel=\"stylesheet\"\n    />\n    <link rel=\"stylesheet\" href=\"styles.css?v=4\" />\n  </head>\n  <body>\n    <div class=\"bg-shapes\">\n      <span></span>\n      <span></span>\n      <span></span>\n    </div>\n\n    <header class=\"hero\">\n      <div>\n        <p class=\"eyebrow\">Quest Friendly • SHARP 3DGS • SBS MP4</p>\n        <h1>2D → 3D 立体视频生成台</h1>\n        <p class=\"subtitle\">\n          高质量、舒适、稳健。先设置参数，再一键生成命令运行。\n        </p>\n      </div>\n      <div class=\"hero-card\">\n        <div class=\"metric\">\n          <span>舒适优先</span>\n          <strong id=\"comfortLabel\">舒适</strong>\n        </div>\n        <div class=\"metric\">\n          <span>最大视差</span>\n          <strong><span id=\"dispValue\">30</span> px</strong>\n        </div>\n        <div class=\"metric\">\n          <span>基线</span>\n          <strong><span id=\"baselineValue\">0.03</span> m</strong>\n        </div>\n      </div>\n    </header>\n\n    <section class=\"preset-row\">\n      <button class=\"preset\" data-preset=\"comfort\">舒适优先</button>\n      <button class=\"preset\" data-preset=\"depth\">立体感更强</button>\n      <button class=\"preset\" data-preset=\"stable\">保守稳定</button>\n    </section>\n\n    <main class=\"grid\">\n      <section class=\"card\">\n        <h2>输入与输出</h2>\n        <div class=\"field\">\n          <label for=\"videoPath\">输入视频</label>\n          <div class=\"picker-row\">\n            <input type=\"text\" id=\"videoPath\" placeholder=\"选择并上传后显示路径\" readonly />\n            <button type=\"button\" class=\"picker-btn\" id=\"videoPickBtn\">选择并上传</button>\n          </div>\n          <input type=\"file\" id=\"videoPicker\" accept=\"video/*\" hidden />\n          <p class=\"hint\" id=\"videoHint\">未选择文件</p>\n        </div>\n        <div class=\"field\">\n          <label for=\"outPath\">输出文件名</label>\n          <div class=\"picker-row\">\n            <input type=\"text\" id=\"outPath\" placeholder=\"output_sbs.mp4\" />\n            <button type=\"button\" class=\"picker-btn\" id=\"downloadBtn\">下载输出</button>\n          </div>\n          <p class=\"hint\" id=\"outHint\">生成完成后可下载保存</p>\n        </div>\n        <label>\n          调试目录（可选）\n          <input type=\"text\" id=\"debugDir\" placeholder=\"debug_output\" />\n        </label>\n        <label>\n          设备\n          <select id=\"device\" disabled>\n            <option value=\"cuda\">cuda (RTX 5070 Ti)</option>\n          </select>\n        </label>\n      </section>\n\n      <section class=\"card\">\n        <h2>舒适与深度</h2>\n        <label>\n          基线 (m)\n          <input type=\"number\" id=\"baseline\" step=\"0.001\" value=\"0.03\" />\n        </label>\n        <label>\n          最小基线 (m)\n          <input type=\"number\" id=\"baselineMin\" step=\"0.001\" value=\"0\" />\n        </label>\n        <label>\n          最大视差 (px)\n          <input type=\"range\" id=\"maxDisp\" min=\"10\" max=\"60\" value=\"30\" />\n          <div class=\"range-info\"><span>10</span><span>60</span></div>\n        </label>\n        <label>\n          水平 FOV (deg)\n          <input type=\"number\" id=\"fov\" step=\"1\" value=\"60\" />\n        </label>\n      </section>\n\n      <section class=\"card\">\n        <h2>稳健与质量</h2>\n        <label>\n          镜头切分阈值\n          <input type=\"number\" id=\"cutThreshold\" step=\"0.01\" value=\"0.6\" />\n        </label>\n        <label>\n          最短镜头长度 (s)\n          <input type=\"number\" id=\"minShot\" step=\"0.1\" value=\"0.5\" />\n        </label>\n        <label>\n          最长镜头长度 (s)\n          <input type=\"number\" id=\"maxShot\" step=\"0.5\" value=\"5\" />\n        </label>\n        <label>\n          PnP 最小内点\n          <input type=\"number\" id=\"minInliers\" step=\"1\" value=\"80\" />\n        </label>\n        <label>\n          PnP 最大重投影误差\n          <input type=\"number\" id=\"maxReproj\" step=\"0.1\" value=\"2.5\" />\n        </label>\n      </section>\n\n      <section class=\"card\">\n        <h2>编码与渲染</h2>\n        <label>\n          ffmpeg CRF\n          <input type=\"number\" id=\"crf\" step=\"1\" value=\"18\" />\n        </label>\n        <label>\n          ffmpeg preset\n          <select id=\"preset\">\n            <option value=\"slow\">slow</option>\n            <option value=\"medium\">medium</option>\n            <option value=\"fast\">fast</option>\n          </select>\n        </label>\n        <label>\n          补洞半径\n          <input type=\"number\" id=\"inpaint\" step=\"1\" value=\"3\" />\n        </label>\n        <label>\n          调试抽帧间隔\n          <input type=\"number\" id=\"debugInterval\" step=\"1\" value=\"30\" />\n        </label>\n        <label>\n          限制帧数（可选）\n          <input type=\"number\" id=\"maxFrames\" step=\"1\" placeholder=\"0\" />\n        </label>\n      </section>\n    </main>\n\n    <section class=\"card command-card\">\n      <div class=\"run-header\">\n        <div>\n          <h2>一键运行</h2>\n          <p class=\"muted\">本地执行并实时显示进度与日志。</p>\n        </div>\n        <span class=\"status-pill\" id=\"statusPill\">空闲</span>\n      </div>\n      <div class=\"progress\">\n        <div class=\"progress-bar\" id=\"progressBar\"></div>\n      </div>\n      <div class=\"progress-info\" id=\"progressInfo\">等待启动</div>\n      <div class=\"actions\">\n        <button class=\"primary\" id=\"runBtn\">开始生成</button>\n        <button class=\"ghost\" id=\"stopBtn\">停止</button>\n        <button class=\"ghost\" id=\"resetBtn\">恢复默认</button>\n      </div>\n      <div class=\"log-box\" id=\"logBox\"></div>\n    </section>\n\n    <footer class=\"footer\">\n      <span>提示：舒适优先时建议 max_disp ≤ 30，立体感增强可适度提高。</span>\n    </footer>\n\n    <script src=\"app.js?v=4\"></script>\n  </body>\n</html>\n"}, {"path": "web/styles.css", "content": ":root {\n  --bg: #f6f1e8;\n  --bg-strong: #efe3d0;\n  --ink: #1c1c1c;\n  --muted: #5f5a54;\n  --accent: #e46a4a;\n  --accent-2: #1c7c7c;\n  --card: #fff9f0;\n  --border: rgba(28, 28, 28, 0.12);\n  --shadow: 0 20px 60px rgba(28, 28, 28, 0.12);\n}\n\n* {\n  box-sizing: border-box;\n}\n\nbody {\n  margin: 0;\n  font-family: \"Space Grotesk\", \"Segoe UI\", sans-serif;\n  color: var(--ink);\n  background: radial-gradient(circle at top left, #fff8ee 0%, #f5e9d6 40%, #f2e2d6 100%);\n  min-height: 100vh;\n  padding: 32px 6vw 64px;\n}\n\n.bg-shapes span {\n  position: fixed;\n  border-radius: 50%;\n  filter: blur(0.5px);\n  opacity: 0.5;\n  animation: float 12s ease-in-out infinite;\n  z-index: 0;\n}\n\n.bg-shapes span:nth-child(1) {\n  width: 320px;\n  height: 320px;\n  background: #f6c8a8;\n  top: -80px;\n  right: -80px;\n}\n\n.bg-shapes span:nth-child(2) {\n  width: 220px;\n  height: 220px;\n  background: #b7dfd6;\n  bottom: 120px;\n  left: -60px;\n  animation-delay: -4s;\n}\n\n.bg-shapes span:nth-child(3) {\n  width: 160px;\n  height: 160px;\n  background: #f1b28e;\n  top: 45%;\n  left: 50%;\n  animation-delay: -2s;\n}\n\n@keyframes float {\n  0%,\n  100% {\n    transform: translate(0, 0);\n  }\n  50% {\n    transform: translate(12px, -14px);\n  }\n}\n\n.hero {\n  position: relative;\n  z-index: 1;\n  display: flex;\n  gap: 32px;\n  align-items: center;\n  justify-content: space-between;\n  flex-wrap: wrap;\n  margin-bottom: 24px;\n  animation: fadeUp 0.8s ease-out;\n}\n\n.eyebrow {\n  text-transform: uppercase;\n  letter-spacing: 0.2em;\n  font-size: 12px;\n  color: var(--muted);\n  margin: 0 0 8px;\n}\n\nh1 {\n  font-size: clamp(28px, 4vw, 46px);\n  margin: 0 0 12px;\n}\n\n.subtitle {\n  margin: 0;\n  color: var(--muted);\n  font-size: 16px;\n  max-width: 520px;\n}\n\n.hero-card {\n  background: linear-gradient(135deg, #fff7ec, #fff0e1);\n  border: 1px solid var(--border);\n  border-radius: 24px;\n  padding: 20px 24px;\n  min-width: 260px;\n  box-shadow: var(--shadow);\n  display: grid;\n  gap: 12px;\n}\n\n.metric {\n  display: flex;\n  justify-content: space-between;\n  align-items: baseline;\n  font-size: 14px;\n  color: var(--muted);\n}\n\n.metric strong {\n  color: var(--ink);\n  font-size: 18px;\n}\n\n.preset-row {\n  position: relative;\n  z-index: 1;\n  display: flex;\n  gap: 12px;\n  flex-wrap: wrap;\n  margin-bottom: 24px;\n}\n\n.preset {\n  padding: 10px 18px;\n  border-radius: 999px;\n  border: 1px solid var(--border);\n  background: var(--card);\n  cursor: pointer;\n  transition: transform 0.2s ease, box-shadow 0.2s ease;\n}\n\n.preset:hover {\n  transform: translateY(-2px);\n  box-shadow: 0 6px 16px rgba(28, 28, 28, 0.12);\n}\n\n.grid {\n  position: relative;\n  z-index: 1;\n  display: grid;\n  grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));\n  gap: 18px;\n  margin-bottom: 24px;\n  animation: fadeUp 0.9s ease-out;\n}\n\n.card {\n  background: var(--card);\n  border: 1px solid var(--border);\n  border-radius: 20px;\n  padding: 18px 20px;\n  box-shadow: 0 12px 30px rgba(28, 28, 28, 0.08);\n}\n\n.card h2 {\n  margin: 0 0 16px;\n  font-size: 18px;\n}\n\nlabel {\n  display: flex;\n  flex-direction: column;\n  gap: 6px;\n  font-size: 13px;\n  color: var(--muted);\n  margin-bottom: 14px;\n}\n\nlabel[for] {\n  margin-bottom: 6px;\n}\n\n.field {\n  display: flex;\n  flex-direction: column;\n  margin-bottom: 14px;\n}\n\n.picker-row {\n  display: grid;\n  grid-template-columns: minmax(0, 1fr) auto;\n  gap: 10px;\n  align-items: center;\n}\n\n.picker-btn {\n  border-radius: 12px;\n  border: 1px solid var(--border);\n  background: #fff3e6;\n  padding: 10px 16px;\n  cursor: pointer;\n  font-size: 13px;\n}\n\n.hint {\n  margin: 6px 0 0;\n  font-size: 12px;\n  color: var(--muted);\n}\n\ninput,\nselect {\n  padding: 10px 12px;\n  border-radius: 12px;\n  border: 1px solid var(--border);\n  font-size: 14px;\n  font-family: inherit;\n  background: #fff;\n}\n\ninput[type=\"range\"] {\n  padding: 0;\n}\n\n.range-info {\n  display: flex;\n  justify-content: space-between;\n  font-size: 11px;\n  color: var(--muted);\n}\n\n.command-card {\n  position: relative;\n  z-index: 1;\n  display: grid;\n  gap: 16px;\n  align-items: center;\n}\n\n.run-header {\n  display: flex;\n  align-items: center;\n  justify-content: space-between;\n  gap: 12px;\n}\n\n.status-pill {\n  padding: 6px 14px;\n  border-radius: 999px;\n  border: 1px solid var(--border);\n  font-size: 12px;\n  background: #f7eee3;\n}\n\n.progress {\n  height: 12px;\n  border-radius: 999px;\n  background: #efe3d2;\n  overflow: hidden;\n}\n\n.progress-bar {\n  height: 100%;\n  width: 0%;\n  background: linear-gradient(90deg, #e46a4a, #f2a266);\n  transition: width 0.3s ease;\n}\n\n.progress-info {\n  font-size: 12px;\n  color: var(--muted);\n}\n\n.log-box {\n  background: #161514;\n  color: #f5f2ee;\n  padding: 14px;\n  border-radius: 14px;\n  font-size: 12px;\n  line-height: 1.5;\n  white-space: pre-wrap;\n  max-height: 240px;\n  overflow: auto;\n}\n\n.actions {\n  display: flex;\n  gap: 12px;\n  flex-wrap: wrap;\n}\n\nbutton.primary {\n  background: var(--accent);\n  color: #fff;\n  border: none;\n  border-radius: 999px;\n  padding: 12px 22px;\n  font-size: 14px;\n  cursor: pointer;\n  box-shadow: 0 12px 30px rgba(228, 106, 74, 0.25);\n  transition: transform 0.2s ease;\n}\n\nbutton.primary:hover {\n  transform: translateY(-1px);\n}\n\nbutton.ghost {\n  background: transparent;\n  border: 1px solid var(--border);\n  border-radius: 999px;\n  padding: 12px 20px;\n  cursor: pointer;\n}\n\n.footer {\n  position: relative;\n  z-index: 1;\n  font-size: 12px;\n  color: var(--muted);\n}\n\n.muted {\n  color: var(--muted);\n  margin: 0;\n}\n\n@keyframes fadeUp {\n  from {\n    opacity: 0;\n    transform: translateY(12px);\n  }\n  to {\n    opacity: 1;\n    transform: translateY(0);\n  }\n}\n\n@media (max-width: 720px) {\n  body {\n    padding: 24px 5vw 48px;\n  }\n  .hero {\n    gap: 20px;\n  }\n  .hero-card {\n    width: 100%;\n  }\n}\n"}, {"path": "web/app.js", "content": "const BUILD_ID = \"2025-12-20-4\";\n\nconst defaults = {\n  baseline: 0.03,\n  baselineMin: 0,\n  maxDisp: 30,\n  fov: 60,\n  cutThreshold: 0.6,\n  minShot: 0.5,\n  maxShot: 5,\n  minInliers: 80,\n  maxReproj: 2.5,\n  crf: 18,\n  preset: \"slow\",\n  inpaint: 3,\n  debugInterval: 30,\n  device: \"cuda\",\n};\n\nconst ids = [\n  \"videoPath\",\n  \"outPath\",\n  \"debugDir\",\n  \"baseline\",\n  \"baselineMin\",\n  \"maxDisp\",\n  \"fov\",\n  \"cutThreshold\",\n  \"minShot\",\n  \"maxShot\",\n  \"minInliers\",\n  \"maxReproj\",\n  \"crf\",\n  \"preset\",\n  \"inpaint\",\n  \"debugInterval\",\n  \"maxFrames\",\n  \"device\",\n];\n\nconst els = Object.fromEntries(ids.map((id) => [id, document.getElementById(id)]));\nconst runBtn = document.getElementById(\"runBtn\");\nconst stopBtn = document.getElementById(\"stopBtn\");\nconst statusPill = document.getElementById(\"statusPill\");\nconst progressBar = document.getElementById(\"progressBar\");\nconst progressInfo = document.getElementById(\"progressInfo\");\nconst logBox = document.getElementById(\"logBox\");\nconst dispValue = document.getElementById(\"dispValue\");\nconst baselineValue = document.getElementById(\"baselineValue\");\nconst comfortLabel = document.getElementById(\"comfortLabel\");\nconst videoPicker = document.getElementById(\"videoPicker\");\nconst videoPickBtn = document.getElementById(\"videoPickBtn\");\nconst downloadBtn = document.getElementById(\"downloadBtn\");\nconst videoHint = document.getElementById(\"videoHint\");\nconst outHint = document.getElementById(\"outHint\");\n\nconst presets = {\n  comfort: {\n    baseline: 0.028,\n    maxDisp: 24,\n    fov: 60,\n    minInliers: 80,\n    maxReproj: 2.2,\n    inpaint: 3,\n  },\n  depth: {\n    baseline: 0.04,\n    maxDisp: 40,\n    fov: 58,\n    minInliers: 70,\n    maxReproj: 2.6,\n    inpaint: 2,\n  },\n  stable: {\n    baseline: 0.025,\n    maxDisp: 20,\n    fov: 62,\n    minInliers: 110,\n    maxReproj: 2.0,\n    inpaint: 4,\n  },\n};\n\nlet stream = null;\nlet backendOk = false;\nlet isRunning = false;\nlet lastOutputName = \"\";\n\nfunction updateComfortLabel(maxDisp) {\n  if (maxDisp <= 28) return \"舒适\";\n  if (maxDisp <= 35) return \"偏强\";\n  return \"强烈\";\n}\n\nfunction buildPayload() {\n  return {\n    video: els.videoPath.value.trim(),\n    out: els.outPath.value.trim(),\n    debug_dir: els.debugDir.value.trim(),\n    baseline_m: parseFloat(els.baseline.value),\n    baseline_min_m: parseFloat(els.baselineMin.value),\n    max_disp_px: parseFloat(els.maxDisp.value),\n    fov_deg: parseFloat(els.fov.value),\n    cut_threshold: parseFloat(els.cutThreshold.value),\n    min_shot_len: parseFloat(els.minShot.value),\n    max_shot_len: parseFloat(els.maxShot.value),\n    min_inliers: parseFloat(els.minInliers.value),\n    max_reproj: parseFloat(els.maxReproj.value),\n    ffmpeg_crf: parseInt(els.crf.value, 10),\n    ffmpeg_preset: els.preset.value,\n    inpaint_radius: parseInt(els.inpaint.value, 10),\n    debug_interval: parseInt(els.debugInterval.value, 10),\n    max_frames: parseInt(els.maxFrames.value, 10) || 0,\n    device: els.device.value,\n  };\n}\n\nfunction appendLog(text) {\n  if (!logBox) return;\n  logBox.textContent += `${text}\\n`;\n  logBox.scrollTop = logBox.scrollHeight;\n}\n\nfunction setStatus(state) {\n  if (!statusPill) return;\n  statusPill.textContent = state;\n}\n\nfunction setProgress(percent, frame, total) {\n  if (progressBar) {\n    progressBar.style.width = percent >= 0 ? `${percent}%` : \"0%\";\n  }\n  if (progressInfo) {\n    if (percent < 0 || !total) {\n      progressInfo.textContent = \"处理中...\";\n    } else {\n      progressInfo.textContent = `进度 ${percent.toFixed(1)}% (${frame}/${total})`;\n    }\n  }\n}\n\nfunction updateUI() {\n  const maxDisp = parseFloat(els.maxDisp.value);\n  const baseline = parseFloat(els.baseline.value);\n  dispValue.textContent = Number.isNaN(maxDisp) ? \"-\" : maxDisp.toFixed(0);\n  baselineValue.textContent = Number.isNaN(baseline) ? \"-\" : baseline.toFixed(3);\n  comfortLabel.textContent = updateComfortLabel(maxDisp || defaults.maxDisp);\n\n  if (videoHint && !els.videoPath.value.trim()) {\n    videoHint.textContent = \"未选择文件\";\n  }\n  if (outHint && !els.outPath.value.trim()) {\n    outHint.textContent = \"请输入输出文件名\";\n  }\n\n  if (runBtn) {\n    const canRun = Boolean(els.videoPath.value.trim() && els.outPath.value.trim());\n    runBtn.disabled = !backendOk || isRunning || !canRun;\n  }\n  if (downloadBtn) {\n    downloadBtn.disabled = !lastOutputName;\n  }\n}\n\nasync function startRun() {\n  if (!backendOk) {\n    appendLog(\"后端未连接，请先启动 python tools/web_server.py\");\n    setStatus(\"未连接\");\n    return;\n  }\n  if (runBtn) runBtn.disabled = true;\n  if (stopBtn) stopBtn.disabled = false;\n  setStatus(\"启动中\");\n  setProgress(0, 0, 0);\n  if (logBox) logBox.textContent = \"\";\n\n  let data;\n  try {\n    const payload = buildPayload();\n    const res = await fetch(\"/api/run\", {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify(payload),\n    });\n    data = await res.json();\n  } catch (err) {\n    appendLog(\"无法连接后端，请先启动本地服务。\");\n    setStatus(\"未连接\");\n    if (runBtn) runBtn.disabled = false;\n    if (stopBtn) stopBtn.disabled = true;\n    return;\n  }\n\n  if (!data.ok) {\n    appendLog(data.error || \"启动失败\");\n    setStatus(\"失败\");\n    if (runBtn) runBtn.disabled = false;\n    if (stopBtn) stopBtn.disabled = true;\n    return;\n  }\n\n  setStatus(\"运行中\");\n  isRunning = true;\n  lastOutputName = \"\";\n  updateUI();\n\n  if (stream) stream.close();\n  stream = new EventSource(\"/api/stream\");\n  stream.onmessage = (event) => {\n    const msg = JSON.parse(event.data);\n    if (msg.type === \"log\") {\n      appendLog(msg.line);\n    }\n    if (msg.type === \"progress\") {\n      setProgress(msg.percent, msg.frame, msg.total);\n    }\n    if (msg.type === \"status\") {\n      setStatus(msg.state);\n      if (msg.state !== \"运行中\") {\n        isRunning = false;\n        if (msg.state === \"完成\") {\n          lastOutputName = els.outPath.value.trim();\n        }\n        if (runBtn) runBtn.disabled = false;\n        if (stopBtn) stopBtn.disabled = true;\n        stream.close();\n        updateUI();\n      }\n    }\n  };\n  stream.onerror = () => {\n    setStatus(\"连接中断\");\n    isRunning = false;\n    if (runBtn) runBtn.disabled = false;\n    if (stopBtn) stopBtn.disabled = true;\n    stream.close();\n    updateUI();\n  };\n}\n\nasync function stopRun() {\n  try {\n    await fetch(\"/api/stop\", { method: \"POST\" });\n    setStatus(\"停止中\");\n  } catch (err) {\n    setStatus(\"未连接\");\n  }\n}\n\nasync function pingBackend() {\n  try {\n    const res = await fetch(\"/api/ping\");\n    const data = await res.json();\n    backendOk = Boolean(data.ok);\n  } catch (err) {\n    backendOk = false;\n  }\n\n  if (!backendOk) {\n    setStatus(\"未连接\");\n    appendLog(\"未检测到后端服务，请在终端运行 python tools/web_server.py\");\n    if (runBtn) runBtn.disabled = true;\n    if (stopBtn) stopBtn.disabled = true;\n  } else {\n    setStatus(\"空闲\");\n    updateUI();\n  }\n}\n\nasync function uploadFile(file) {\n  const form = new FormData();\n  form.append(\"file\", file, file.name);\n\n  return new Promise((resolve, reject) => {\n    const xhr = new XMLHttpRequest();\n    xhr.open(\"POST\", \"/api/upload\");\n    xhr.upload.onprogress = (event) => {\n      if (event.lengthComputable && videoHint) {\n        const percent = (event.loaded / event.total) * 100;\n        videoHint.textContent = `上传中 ${percent.toFixed(1)}%`;\n      }\n    };\n    xhr.onload = () => {\n      try {\n        const data = JSON.parse(xhr.responseText);\n        resolve(data);\n      } catch (err) {\n        reject(err);\n      }\n    };\n    xhr.onerror = () => reject(new Error(\"upload failed\"));\n    xhr.send(form);\n  });\n}\n\nfunction pickInputFile() {\n  if (videoPicker) {\n    videoPicker.click();\n  }\n}\n\nif (videoPickBtn) {\n  videoPickBtn.addEventListener(\"click\", () => pickInputFile());\n}\n\nif (videoPicker) {\n  videoPicker.addEventListener(\"change\", (event) => {\n    const file = event.target.files && event.target.files[0];\n    if (!file) return;\n    if (videoHint) {\n      videoHint.textContent = `准备上传 ${file.name}`;\n    }\n    uploadFile(file)\n      .then((data) => {\n        if (!data.ok) {\n          appendLog(data.error || \"上传失败\");\n          if (videoHint) {\n            videoHint.textContent = \"上传失败\";\n          }\n          return;\n        }\n        els.videoPath.value = data.path || \"\";\n        if (data.suggested_output) {\n          els.outPath.value = data.suggested_output;\n        } else if (!els.outPath.value.trim()) {\n          els.outPath.value = `output_${Date.now()}.mp4`;\n        }\n        if (videoHint) {\n          videoHint.textContent = `已上传：${data.name || file.name}`;\n        }\n        updateUI();\n      })\n      .catch(() => {\n        appendLog(\"上传失败，请重试。\");\n        if (videoHint) {\n          videoHint.textContent = \"上传失败\";\n        }\n      });\n  });\n}\n\nif (downloadBtn) {\n  downloadBtn.addEventListener(\"click\", async () => {\n    try {\n      const res = await fetch(\"/api/download\");\n      if (!res.ok) {\n        appendLog(\"下载失败：输出文件不存在\");\n        return;\n      }\n      const blob = await res.blob();\n      const url = URL.createObjectURL(blob);\n      const link = document.createElement(\"a\");\n      link.href = url;\n      link.download = lastOutputName || \"output_sbs.mp4\";\n      document.body.appendChild(link);\n      link.click();\n      link.remove();\n      URL.revokeObjectURL(url);\n    } catch (err) {\n      appendLog(\"下载失败，请重试。\");\n    }\n  });\n}\n\nids.forEach((id) => {\n  els[id].addEventListener(\"input\", updateUI);\n  els[id].addEventListener(\"change\", updateUI);\n});\n\nArray.from(document.querySelectorAll(\".preset\")).forEach((btn) => {\n  btn.addEventListener(\"click\", () => {\n    const preset = presets[btn.dataset.preset];\n    if (!preset) return;\n    Object.entries(preset).forEach(([key, value]) => {\n      const el = els[key];\n      if (el) {\n        el.value = value;\n      }\n    });\n    updateUI();\n  });\n});\n\nconst resetBtn = document.getElementById(\"resetBtn\");\nresetBtn.addEventListener(\"click\", () => {\n  els.videoPath.value = \"\";\n  els.outPath.value = \"\";\n  els.debugDir.value = \"\";\n  Object.entries(defaults).forEach(([key, value]) => {\n    if (els[key]) {\n      els[key].value = value;\n    }\n  });\n  els.maxFrames.value = \"\";\n  lastOutputName = \"\";\n  updateUI();\n});\n\nif (runBtn) runBtn.addEventListener(\"click\", () => startRun().catch(() => {}));\nif (stopBtn) stopBtn.addEventListener(\"click\", () => stopRun().catch(() => {}));\n\nupdateUI();\nif (stopBtn) stopBtn.disabled = true;\nif (downloadBtn) downloadBtn.disabled = true;\nappendLog(`前端已加载 ${BUILD_ID}`);\n\npingBackend();"}]